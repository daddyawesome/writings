{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETLExcel to Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrNlwnCgIIrPG+kdE/fDjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daddyawesome/writings/blob/master/ETLExcel_to_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOlVL__S7R-L"
      },
      "source": [
        "# From Excel To Databases with Python\r\n",
        "\r\n",
        "As a business analyst, not a day goes by that I do not find myself fiddling with some data in an excel spreadsheet. Talk to anyone working in an analytical role, and they will tell you about their love-hate relationship with excel. For all the good things that excel can do; it’s simply a pain to work with when it comes to larger data sets.   \r\n",
        "<br>\r\n",
        "Pivots take forever to load, the machine runs out of memory, and before you know it, the whole thing becomes unmanageable. Not to mention that excel can only support up to 1,048,576 rows. Sure, you could consider doing things in VBA, but what’s the point.\r\n",
        "<br>\r\n",
        "If only there were an easy way to transfer data into a SQL Database, do your analysis and then delete it all. Well, this is where Python swoops in to save the day.\r\n",
        "<br>\r\n",
        "## SQL In Python\r\n",
        "To begin with, let us explore the most popular options when it comes to SQL in Python. The two most popular SQL DBs to work within Python is MySQL and SQLite.\r\n",
        "MySQL has two popular libraries associated with it: PyMySQL and MySQLDb; while SQLite has SQLite3.   \r\n",
        "<br>\r\n",
        "SQLite is what is known as an embedded database, which means it runs within our application and hence it is not required to be installed somewhere first (unlike MySQL).   \r\n",
        "<br>\r\n",
        "This is a significant difference; and pivotal in our quest for quick data analysis. As such, we will go ahead and learn how to use SQLite.\r\n",
        "\r\n",
        "## Setting up SQLite in Python\r\n",
        "The first thing we need to do is import the library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxbCGrg-68q5"
      },
      "source": [
        "import sqlite3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOVVEOIe736i"
      },
      "source": [
        "Then we need to determine whether we would like to save this database anywhere or simply hold it in memory while our application is running.   \r\n",
        "<br>\r\n",
        "If decided to actually save the database down with any of the data imported, we would then have to give the DB a name, say ‘FinanceExplainedDb’, and have the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7r-ErUS79Cx"
      },
      "source": [
        "dbname = 'FinanceExplainedDb'\r\n",
        "conn = sqlite3.connect(dbname + '.sqlite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQEE2l_W8Abp"
      },
      "source": [
        "On the other hand, if we wanted the whole thing in memory, and for it to vanish when we were done, we could use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EtRwjar8FVI"
      },
      "source": [
        "conn = sqlite3.connect(':memory:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV3akIg-8PBY"
      },
      "source": [
        "At this point, SQLite is all set up and ready to be used in Python. Assuming we had some data loaded in the DB under `Table1`, we could execute SQL commands in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJWavkZ8Kjv"
      },
      "source": [
        "cur = conn.cursor()\r\n",
        "cur.execute('SELECT * FROM Table1')\r\n",
        "for row in cur:\r\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrEVR5J8ZG4"
      },
      "source": [
        "Let us now explore how we can make our data available through our application using Pandas.\r\n",
        "\r\n",
        "## Using Pandas to load data in our application\r\n",
        "Assuming that we already have the data, we would like to analyse, we can use the Python Pandas library to do it.   \r\n",
        "<br>\r\n",
        "First, we need to import the Pandas library and then we can load the data in a data frame (You can think of data frames as an array of sorts):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlWO67l68flp"
      },
      "source": [
        "import pandas as pd\r\n",
        "#if we have a csv file\r\n",
        "df = pd.read_csv('ourfile.csv')\r\n",
        "#if we have an excel file\r\n",
        "df = pd.read_excel('ourfile.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqNkQ-HP8hzw"
      },
      "source": [
        "Once we have loaded the data, we can put it straight into our SQL Database with a simple command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpm_oHlZ8msg"
      },
      "source": [
        "df.to_sql(name='Table1', con=conn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqXNPNEW8nso"
      },
      "source": [
        "If you are loading multiple files within the same table, you can use the `if_exists` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwu4M6D38tZY"
      },
      "source": [
        "df.to_sql(name='Table1', con=conn, if_exists='append')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydad-AIW8vU4"
      },
      "source": [
        "## Memory Considerations\r\n",
        "When it comes to dealing with larger data sets, we will not be able to use this one-line command to load the data. Our application will run out of memory.   \r\n",
        "<br>\r\n",
        "Instead, we will have to load our data little by little.  \r\n",
        "<br>\r\n",
        "For this example, let’s assume we will load 10,000 rows at a time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZCOkMlY89_Q"
      },
      "source": [
        "chunksize = 10000\r\n",
        "for chunk in pd.read_csv('ourfile.csv', chunksize=chunksize):\r\n",
        "    chunk.to_sql(name='Table1', con=conn, if_exists='append')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BfM2JVD9IAn"
      },
      "source": [
        "## Bringing it all together\r\n",
        "To bring everything together, I have decided to give you a Python script that covers most of the things we talked about.   \r\n",
        "\r\n",
        "The script will do the following things:\r\n",
        "- Load some sample data from a Python library\r\n",
        "- Write the data out to a CSV\r\n",
        "- Load the data back into our application through the CSV in a data frame chunk by chunk and put in a DB\r\n",
        "- Then execute a SELECT statement on the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktfaeq3i8-z_"
      },
      "source": [
        "import sqlite3, pandas as pd, numpy as np\r\n",
        "#####Creating test data for us -- you can ignore\r\n",
        "from sklearn import datasets\r\n",
        "iris = datasets.load_iris()\r\n",
        "df1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\r\n",
        "df1.to_csv('TestData.csv',index=False)\r\n",
        "###########################\r\n",
        "conn = sqlite3.connect(':memory:')\r\n",
        "cur = conn.cursor()\r\n",
        "chunksize = 10\r\n",
        "for chunk in pd.read_csv('TestData.csv', chunksize=chunksize):\r\n",
        "    chunk.columns = chunk.columns.str.replace(' ', '_') #replacing spaces with underscores for column names\r\n",
        "    chunk.to_sql(name='Table1', con=conn, if_exists='append')\r\n",
        "cur.execute('SELECT * FROM Table1')\r\n",
        "names = list(map(lambda x: x[0], cur.description)) #Returns the column names\r\n",
        "print(names)\r\n",
        "for row in cur:\r\n",
        "    print(row)\r\n",
        "cur.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFWbbTvw9gMf"
      },
      "source": [
        "There you have it guys — a brief introduction on how you can import your data into a database for quick analysis."
      ]
    }
  ]
}